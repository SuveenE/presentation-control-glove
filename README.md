# Presentation Control Glove with Real-time Gesture Recognition with FPGA Acceleration

> A complete end-to-end system for real-time hand gesture recognition using dual IMU sensors, deployed on FPGA hardware for ultra-low latency inference. This includes code used in data collection, preprocessing, training on Google Colab and files needed to create IP block to run inference on FPGA.

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/)
[![Dataset](https://img.shields.io/badge/dataset-Kaggle-20BEFF.svg)](https://www.kaggle.com/datasets/suveenellawela/hand-gesture-classification-2-imu-glove)

## ğŸ¯ Overview

This project implements a real-time gesture recognition system using data from two IMU (Inertial Measurement Unit) sensors - one on the wrist and one on the index finger. The system:

- Captures 6-axis IMU data (3-axis accelerometer + 3-axis gyroscope) from 2 sensors
- Extracts 84 hand-crafted features from 1-second windows
- Classifies gestures using a Multi-Layer Perceptron (MLP) neural network
- Achieves **sub 3 ms inference time** on FPGA hardware (Xilinx Ultra96/ZCU104)
- Recognizes **8 gesture classes** with **99% accuracy**

### Supported Gestures
0 - NONE
1 - SLIDE_LEFT
2 - SLIDE_RIGHT
3 - WRIST_TURN_CLOCKWISE
4 - WRIST_TURN_ANTI_CLOCKWISE
5 - SLIDE_UP
6 - SLIDE_DOWN
7 - SHAKE

## ğŸ“ Repository Structure

```
gesture-recognition-fpga/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ model_info.json                    # Model metadata and performance metrics
â”‚
â”œâ”€â”€ model/                             # Trained model artifacts
â”‚   â”œâ”€â”€ weights_npy/                   # NumPy weight files (8 files)
â”‚   â”‚   â”œâ”€â”€ w0.npy, b0.npy            # Layer 1 weights & biases
â”‚   â”‚   â”œâ”€â”€ w1.npy, b1.npy            # Layer 2 weights & biases
â”‚   â”‚   â”œâ”€â”€ w2.npy, b2.npy            # Layer 3 weights & biases
â”‚   â”‚   â””â”€â”€ w3.npy, b3.npy            # Output layer weights & biases
â”‚   â”œâ”€â”€ mlp_weights.h                  # C header with weights
â”‚   â”œâ”€â”€ mlp_test_data.h               # Test data for validation
â”‚   â””â”€â”€ scaler.pkl                     # StandardScaler for preprocessing
â”‚
â”œâ”€â”€ notebooks/                         # Jupyter notebooks
â”‚   â”œâ”€â”€ train_model.ipynb             # Model training pipeline
â”‚   â””â”€â”€ evaluate_model.ipynb          # Model evaluation and testing
â”‚
â”œâ”€â”€ src/                               # Python source code
â”‚   â”œâ”€â”€ preprocess.py                 # Feature extraction (84 features)
â”‚   â”œâ”€â”€ segment_gestures.py           # Gesture segmentation from streams
â”‚   â””â”€â”€ extract_weights.py            # Convert model to C headers
â”‚
â””â”€â”€ hardware/                          # FPGA/embedded implementation
    â”œâ”€â”€ mlp_model.cpp                 # C++ inference implementation
    â”œâ”€â”€ mlp_model.h                   # C++ header
    â”œâ”€â”€ mlp_model_test.cpp            # C++ test harness
    â”œâ”€â”€ mlp_weights.h                 # Model weights (C header)
    â”œâ”€â”€ mlp_test_data.h               # Test data (C header)
    â””â”€â”€ bitstream/                    # FPGA bitstream files
        â”œâ”€â”€ mlp.bit                   # FPGA bitstream
        â”œâ”€â”€ mlp.hwh                   # Hardware handoff file
        â””â”€â”€ mlp.xsa                   # Xilinx System Archive
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/SuveenE/presentation-control-glove.git
cd presentation-control-glove
```

2. **Install Python dependencies**
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip3 install -r requirements.txt
```

3. **Download the dataset**
- Dataset is available on Kaggle: [https://www.kaggle.com/datasets/suveenellawela/hand-gesture-classification-2-imu-glove]

## ğŸ“Š Dataset and Data Format

### Dataset
The training dataset is available on Kaggle: **[Link to your Kaggle dataset]**

**Dataset Statistics:**
- Total gesture windows: X,XXX
- Classes: 8 (7 gestures + null/none class)
- Sampling rate: ~50 Hz
- Window duration: 1 second (~50 samples per gesture)
- Subjects: X participants
- Train/Val/Test split: XX% / XX% / XX%

### Raw IMU Data Format

Each gesture sample is stored as a CSV file with the following structure:

**Columns (13 total):**
```
timestamp, 
Imu0_linear_accleration_x, Imu0_linear_accleration_y, Imu0_linear_accleration_z,
Imu0_angular_velocity_x, Imu0_angular_velocity_y, Imu0_angular_velocity_z,
Imu1_linear_accleration_x, Imu1_linear_accleration_y, Imu1_linear_accleration_z,
Imu1_angular_velocity_x, Imu1_angular_velocity_y, Imu1_angular_velocity_z
```

**Note:** The typo "accleration" (instead of "acceleration") is intentional and preserved for consistency with the data collection system.

**Column Details:**
- `timestamp`: Time in milliseconds (ESP32 clock)
- `Imu0_*`: Wrist IMU data
  - `linear_accleration_[x,y,z]`: Acceleration in m/sÂ² (range: Â±156.96)
  - `angular_velocity_[x,y,z]`: Gyroscope in deg/s (range: Â±2000)
- `Imu1_*`: Index finger IMU data (same format as IMU0)

**Example CSV:**
```csv
timestamp,Imu0_linear_accleration_x,Imu0_linear_accleration_y,...
1234567,0.12,9.81,0.03,-1.5,2.3,0.8,0.15,9.78,0.05,-1.2,2.5,0.9
1234587,0.14,9.83,0.04,-1.6,2.4,0.7,0.17,9.80,0.06,-1.3,2.6,0.8
...
```
## ğŸ§  Model Details

### Architecture
```
Input (84) â†’ Dense(64, ReLU) â†’ Dense(32, ReLU) â†’ Dense(8, Softmax)
```

### Feature Engineering (84 features)
- **Per-IMU features (40 Ã— 2 = 80)**:
  - Axis statistics (5 stats Ã— 3 axes Ã— 2 signals): mean, std, RMS, max, median
    - 3 accel axes (x, y, z)
    - 3 gyro axes (x, y, z)
  - Magnitude statistics (5 stats Ã— 2 magnitudes): accel magnitude, gyro magnitude

- **Cross-IMU features (4)**:
  - Acceleration magnitude correlation (wrist â†” finger)
  - Gyroscope magnitude correlation (wrist â†” finger)
  - RMS ratio: finger accel / wrist accel
  - RMS ratio: finger gyro / wrist gyro

### Training Details
- **Optimizer**: Adam
- **Loss**: Categorical Crossentropy
- **Batch Size**: 32
- **Epochs**: 100 (with early stopping)
- **Regularization**: Dropout (0.3)
- **Data Augmentation**: [Describe any augmentation used]

### Performance Metrics
| Metric | Value |
|--------|-------|
| Test Accuracy | XX.X% |
| Precision (macro avg) | XX.X% |
| Recall (macro avg) | XX.X% |
| F1-Score (macro avg) | XX.X% |
| Inference Time (FPGA) | X.X ms |
| Model Size | XX KB |

## âš¡ Hardware Deployment (FPGA)

The model is deployed on Xilinx FPGA for ultra-low latency inference. The repository includes:
- **C++ implementation** (`mlp_model.cpp/.h`) - Pure C++ inference without dependencies
- **Pre-compiled bitstream** (`hardware/bitstream/`) - Ready to deploy on PYNQ boards
- **Test harness** (`mlp_model_test.cpp`) - Validates C++ implementation against known outputs

## ğŸ“ Citation

If you use this work in your research or project, please cite:

```bibtex
@software{gesture_recognition_fpga_2025,
  author = {Suveen Ellawela},
  title = {Presentation Control Glove with Real-time Gesture Recognition with FPGA Acceleration},
  year = {2025},
  url = {https://github.com/SuveenE/presentation-control-glove},
  note = {Dataset: https://www.kaggle.com/datasets/suveenellawela/hand-gesture-classification-2-imu-glove}
}
```

## ğŸ“„ License

This project is licensed under the MIT License. See LICENSE file for details.

## ğŸ™ Acknowledgments

- Dataset collected as part of CG4002 Embedded Systems Design Project, NUS
- FPGA deployment using Xilinx Vivado HLS and PYNQ framework
- IMU sensors: MPU6050 (wrist and index finger)

## ğŸ“§ Contact

- **GitHub**: https://github.com/SuveenE/
- **Email**: suveen.te1[at]gmail.com

---
